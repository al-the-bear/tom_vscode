/// AI Prompt API — Local LLM / Prompt Expander
///
/// Provides Dart access to the DartScript VS Code extension's prompt
/// expansion feature, which uses a local Ollama model to expand short
/// prompts into detailed, high-quality prompts.
///
/// The API uses the `localLlm.*Vce` bridge methods, so no inline JS
/// scripts are needed — all calls go directly to the extension handler.
///
/// ## Quick Start
///
/// ```dart
/// import 'package:tom_vscode_scripting_api/tom_vscode_scripting_api.dart';
///
/// void main() async {
///   final result = await AiPromptApi.process('add error handling');
///   print(result.result); // expanded prompt
/// }
/// ```
library;

import 'vscode.dart';
import 'vscode_adapter.dart';

// ============================================================================
// Return Types
// ============================================================================

/// Token usage statistics from a local Ollama model call.
class AiTokenStats {
  /// Number of tokens in the prompt sent to the model.
  final int promptTokens;

  /// Number of tokens generated by the model.
  final int completionTokens;

  /// Total wall-clock duration in milliseconds.
  final double totalDurationMs;

  /// Time spent loading the model in milliseconds.
  final double loadDurationMs;

  AiTokenStats({
    required this.promptTokens,
    required this.completionTokens,
    required this.totalDurationMs,
    required this.loadDurationMs,
  });

  factory AiTokenStats.fromJson(Map<String, dynamic> json) {
    return AiTokenStats(
      promptTokens: (json['promptTokens'] as num?)?.toInt() ?? 0,
      completionTokens: (json['completionTokens'] as num?)?.toInt() ?? 0,
      totalDurationMs: (json['totalDurationMs'] as num?)?.toDouble() ?? 0,
      loadDurationMs: (json['loadDurationMs'] as num?)?.toDouble() ?? 0,
    );
  }

  Map<String, dynamic> toJson() => {
        'promptTokens': promptTokens,
        'completionTokens': completionTokens,
        'totalDurationMs': totalDurationMs,
        'loadDurationMs': loadDurationMs,
      };

  @override
  String toString() =>
      'AiTokenStats(prompt: $promptTokens, completion: $completionTokens, '
      'duration: ${totalDurationMs.toStringAsFixed(0)}ms)';
}

/// Result of a prompt expansion via [AiPromptApi.process].
class AiPromptResult {
  /// Whether the expansion succeeded.
  final bool success;

  /// The final expanded text after template application.
  final String result;

  /// The raw LLM response before any processing.
  final String rawResponse;

  /// The cleaned response (after think-tag stripping).
  final String response;

  /// Extracted `<think>` tag content, if any.
  final String thinkTagContent;

  /// Profile key used for this expansion.
  final String profile;

  /// Model config key used for this expansion.
  final String modelConfig;

  /// Error message if [success] is false.
  final String? error;

  /// Token usage statistics (null if unavailable).
  final AiTokenStats? tokenInfo;

  AiPromptResult({
    required this.success,
    required this.result,
    required this.rawResponse,
    required this.response,
    required this.thinkTagContent,
    required this.profile,
    required this.modelConfig,
    this.error,
    this.tokenInfo,
  });

  factory AiPromptResult.fromJson(Map<String, dynamic> json) {
    return AiPromptResult(
      success: json['success'] as bool? ?? false,
      result: json['result'] as String? ?? '',
      rawResponse: json['rawResponse'] as String? ?? '',
      response: json['response'] as String? ?? '',
      thinkTagContent: json['thinkTagContent'] as String? ?? '',
      profile: json['profile'] as String? ?? '',
      modelConfig: json['modelConfig'] as String? ?? '',
      error: json['error'] as String?,
      tokenInfo: json['tokenInfo'] is Map<String, dynamic>
          ? AiTokenStats.fromJson(json['tokenInfo'] as Map<String, dynamic>)
          : null,
    );
  }

  Map<String, dynamic> toJson() => {
        'success': success,
        'result': result,
        'rawResponse': rawResponse,
        'response': response,
        'thinkTagContent': thinkTagContent,
        'profile': profile,
        'modelConfig': modelConfig,
        if (error != null) 'error': error,
        if (tokenInfo != null) 'tokenInfo': tokenInfo!.toJson(),
      };

  @override
  String toString() =>
      'AiPromptResult(success: $success, profile: $profile, '
      'result: ${result.length} chars'
      '${error != null ? ", error: $error" : ""})';
}

/// A prompt expander profile.
class AiPromptProfile {
  /// Profile key identifier.
  final String key;

  /// Human-readable label.
  final String label;

  /// Whether this is the default profile.
  final bool isDefault;

  /// System prompt used for this profile.
  final String? systemPrompt;

  /// Result template (may contain `${result}` placeholder).
  final String? resultTemplate;

  /// Temperature override (null → inherit from top-level config).
  final double? temperature;

  /// Model config key override (null → default model).
  final String? modelConfig;

  AiPromptProfile({
    required this.key,
    required this.label,
    this.isDefault = false,
    this.systemPrompt,
    this.resultTemplate,
    this.temperature,
    this.modelConfig,
  });

  factory AiPromptProfile.fromJson(Map<String, dynamic> json) {
    return AiPromptProfile(
      key: json['key'] as String? ?? '',
      label: json['label'] as String? ?? '',
      isDefault: json['isDefault'] as bool? ?? false,
      systemPrompt: json['systemPrompt'] as String?,
      resultTemplate: json['resultTemplate'] as String?,
      temperature: (json['temperature'] as num?)?.toDouble(),
      modelConfig: json['modelConfig'] as String?,
    );
  }

  Map<String, dynamic> toJson() => {
        'key': key,
        'label': label,
        'isDefault': isDefault,
        if (systemPrompt != null) 'systemPrompt': systemPrompt,
        if (resultTemplate != null) 'resultTemplate': resultTemplate,
        if (temperature != null) 'temperature': temperature,
        if (modelConfig != null) 'modelConfig': modelConfig,
      };

  @override
  String toString() => 'AiPromptProfile(key: $key, label: $label'
      '${isDefault ? ", default" : ""})';
}

/// An Ollama model configuration.
class AiModelConfig {
  /// Model config key identifier.
  final String key;

  /// Ollama server URL.
  final String ollamaUrl;

  /// Ollama model name (e.g. `qwen3:8b`).
  final String model;

  /// Temperature for this model config.
  final double temperature;

  /// Whether to strip `<think>` tags from output.
  final bool stripThinkingTags;

  /// Human-readable description.
  final String? description;

  /// Whether this is the default model config.
  final bool isDefault;

  AiModelConfig({
    required this.key,
    required this.ollamaUrl,
    required this.model,
    required this.temperature,
    required this.stripThinkingTags,
    this.description,
    this.isDefault = false,
  });

  factory AiModelConfig.fromJson(Map<String, dynamic> json) {
    return AiModelConfig(
      key: json['key'] as String? ?? '',
      ollamaUrl: json['ollamaUrl'] as String? ?? 'http://localhost:11434',
      model: json['model'] as String? ?? '',
      temperature: (json['temperature'] as num?)?.toDouble() ?? 0.5,
      stripThinkingTags: json['stripThinkingTags'] as bool? ?? true,
      description: json['description'] as String?,
      isDefault: json['isDefault'] as bool? ?? false,
    );
  }

  Map<String, dynamic> toJson() => {
        'key': key,
        'ollamaUrl': ollamaUrl,
        'model': model,
        'temperature': temperature,
        'stripThinkingTags': stripThinkingTags,
        if (description != null) 'description': description,
        'isDefault': isDefault,
      };

  @override
  String toString() => 'AiModelConfig(key: $key, model: $model'
      '${isDefault ? ", default" : ""})';
}

/// Result of listing available models, including the effective default.
class AiModelsResult {
  /// All configured model configs.
  final List<AiModelConfig> models;

  /// The effective default model config (synthesized from top-level settings).
  final AiModelConfig? effectiveDefault;

  AiModelsResult({required this.models, this.effectiveDefault});

  factory AiModelsResult.fromJson(Map<String, dynamic> json) {
    final models = (json['models'] as List?)
            ?.map((m) => AiModelConfig.fromJson(m as Map<String, dynamic>))
            .toList() ??
        [];
    final effectiveDefault = json['effectiveDefault'] is Map<String, dynamic>
        ? AiModelConfig.fromJson({
            'key': '_default',
            ...(json['effectiveDefault'] as Map<String, dynamic>),
          })
        : null;
    return AiModelsResult(models: models, effectiveDefault: effectiveDefault);
  }
}

// ============================================================================
// AI Prompt API — Static Methods
// ============================================================================

/// AI Prompt API — local LLM prompt expansion.
///
/// Provides type-safe Dart access to the DartScript Prompt Expander, which
/// uses a local Ollama model to expand short prompts into detailed ones.
///
/// All methods are static and require [VSCode.initialize] to have been
/// called first (the adapter is obtained from the singleton).
///
/// ## Examples
///
/// ```dart
/// // Expand a prompt with the default profile
/// final result = await AiPromptApi.process('refactor the auth module');
/// if (result.success) {
///   print(result.result);
/// }
///
/// // Use a specific profile and model
/// final result2 = await AiPromptApi.process(
///   'add error handling',
///   profile: 'detailed',
///   model: 'qwen3:14b',
/// );
///
/// // List available profiles
/// final profiles = await AiPromptApi.getProfiles();
/// for (final p in profiles) {
///   print('${p.key}: ${p.label}${p.isDefault ? " (default)" : ""}');
/// }
/// ```
class AiPromptApi {
  AiPromptApi._();

  /// Get the adapter from the VSCode singleton.
  static VSCodeAdapter get _adapter => VSCode.instance.adapter;

  // --------------------------------------------------------------------------
  // Core
  // --------------------------------------------------------------------------

  /// Expand a prompt using the local Ollama model.
  ///
  /// [prompt] — the short prompt text to expand (required).
  /// [profile] — profile key to use (null → default profile).
  /// [model] — model config key to use (null → default model).
  /// [timeoutSeconds] — max wait time (default 120s).
  ///
  /// Returns an [AiPromptResult] with the expanded text and metadata.
  static Future<AiPromptResult> process(
    String prompt, {
    String? profile,
    String? model,
    int timeoutSeconds = 120,
  }) async {
    final result = await _adapter.sendRequest(
      'localLlm.processVce',
      {
        'prompt': prompt,
        'profile': ?profile,
        'model': ?model,
      },
      scriptName: 'AiPromptApi.process',
      timeout: Duration(seconds: timeoutSeconds),
    );
    return AiPromptResult.fromJson(result);
  }

  // --------------------------------------------------------------------------
  // Profiles
  // --------------------------------------------------------------------------

  /// List all available prompt expander profiles.
  ///
  /// Returns a list of [AiPromptProfile] objects with key, label, defaults,
  /// and optional overrides.
  static Future<List<AiPromptProfile>> getProfiles({
    int timeoutSeconds = 30,
  }) async {
    final result = await _adapter.sendRequest(
      'localLlm.getProfilesVce',
      {},
      scriptName: 'AiPromptApi.getProfiles',
      timeout: Duration(seconds: timeoutSeconds),
    );
    final profiles = result['profiles'] as List?;
    return profiles
            ?.map((p) =>
                AiPromptProfile.fromJson(p as Map<String, dynamic>))
            .toList() ??
        [];
  }

  /// Create or update a prompt expander profile.
  ///
  /// [key] — the profile key (creates if new, updates if existing).
  /// [profile] — the profile data as a map. Must include at least `label`.
  ///
  /// Example:
  /// ```dart
  /// await AiPromptApi.updateProfile('myProfile', {
  ///   'label': 'My Custom Profile',
  ///   'systemPrompt': 'You are a code reviewer...',
  ///   'temperature': 0.3,
  /// });
  /// ```
  static Future<Map<String, dynamic>> updateProfile(
    String key,
    Map<String, dynamic> profile, {
    int timeoutSeconds = 30,
  }) async {
    return await _adapter.sendRequest(
      'localLlm.updateProfileVce',
      {'key': key, 'profile': profile},
      scriptName: 'AiPromptApi.updateProfile',
      timeout: Duration(seconds: timeoutSeconds),
    );
  }

  /// Remove a prompt expander profile.
  ///
  /// [key] — the profile key to remove.
  static Future<Map<String, dynamic>> removeProfile(
    String key, {
    int timeoutSeconds = 30,
  }) async {
    return await _adapter.sendRequest(
      'localLlm.removeProfileVce',
      {'key': key},
      scriptName: 'AiPromptApi.removeProfile',
      timeout: Duration(seconds: timeoutSeconds),
    );
  }

  // --------------------------------------------------------------------------
  // Models
  // --------------------------------------------------------------------------

  /// List all configured Ollama model configs.
  ///
  /// Returns an [AiModelsResult] containing the model list and the
  /// effective default model config.
  static Future<AiModelsResult> getModels({
    int timeoutSeconds = 30,
  }) async {
    final result = await _adapter.sendRequest(
      'localLlm.getModelsVce',
      {},
      scriptName: 'AiPromptApi.getModels',
      timeout: Duration(seconds: timeoutSeconds),
    );
    return AiModelsResult.fromJson(result);
  }

  /// Create or update an Ollama model configuration.
  ///
  /// [key] — the model config key.
  /// [model] — the model config data as a map.
  ///
  /// Example:
  /// ```dart
  /// await AiPromptApi.updateModel('fast', {
  ///   'model': 'qwen3:1.7b',
  ///   'temperature': 0.7,
  ///   'description': 'Fast model for quick expansions',
  /// });
  /// ```
  static Future<Map<String, dynamic>> updateModel(
    String key,
    Map<String, dynamic> model, {
    int timeoutSeconds = 30,
  }) async {
    return await _adapter.sendRequest(
      'localLlm.updateModelVce',
      {'key': key, 'model': model},
      scriptName: 'AiPromptApi.updateModel',
      timeout: Duration(seconds: timeoutSeconds),
    );
  }

  /// Remove an Ollama model configuration.
  ///
  /// [key] — the model config key to remove.
  static Future<Map<String, dynamic>> removeModel(
    String key, {
    int timeoutSeconds = 30,
  }) async {
    return await _adapter.sendRequest(
      'localLlm.removeModelVce',
      {'key': key},
      scriptName: 'AiPromptApi.removeModel',
      timeout: Duration(seconds: timeoutSeconds),
    );
  }
}
